{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37114c77",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d19a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 20:57:53.309051: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-29 20:57:53.636946: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-29 20:57:53.897117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753833474.125815   17210 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753833474.197771   17210 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753833474.702713   17210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753833474.702757   17210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753833474.702760   17210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753833474.702763   17210 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-29 20:57:54.766460: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6169395f",
   "metadata": {},
   "source": [
    "Baixar gloVe em http://nlp.stanford.edu/data/glove.840B.300d.zip e adicionar nesse diret√≥rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc4a4a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa3459e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d246298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path=\"glove.840B.300d.txt\"):\n",
    "    embeddings = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_embeddings = load_glove_embeddings()\n",
    "# with gzip.open(\"glove.840B.300d.pkl.gz\", 'wb') as f:\n",
    "#         pickle.dump(glove_embeddings, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc0f9895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached GloVe embeddings...\n"
     ]
    }
   ],
   "source": [
    "with gzip.open(\"glove.840B.300d.pkl.gz\", 'rb') as f:\n",
    "            print(\"Loading cached GloVe embeddings...\")\n",
    "            GLOVE_EMBEDDINGS = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d6f3317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_dataset(folder, X_train, X_val, X_test, glove_embeddings, max_len=100):\n",
    "    embedding_dim = 300\n",
    "\n",
    "    tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    with open(f\"{folder}/tokenizer.pkl\", \"wb\") as f:\n",
    "        pickle.dump(tokenizer, f)\n",
    "\n",
    "    # Convert to sequences\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_val_seq   = tokenizer.texts_to_sequences(X_val)\n",
    "    X_test_seq  = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "    X_val_pad   = pad_sequences(X_val_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "    X_test_pad  = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    np.save(f\"{folder}/X_train_pad.npy\", X_train_pad)\n",
    "    np.save(f\"{folder}/X_val_pad.npy\", X_val_pad)\n",
    "    np.save(f\"{folder}/X_test_pad.npy\", X_test_pad)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    vocab_size = len(word_index) + 1  # +1 for padding token\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        vector = glove_embeddings.get(word)\n",
    "        if vector is not None:\n",
    "            embedding_matrix[i] = vector\n",
    "\n",
    "    np.save(f\"{folder}/embedding_matrix.npy\", embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601276b8",
   "metadata": {},
   "source": [
    "# Rotten Tomatoes Movie Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0a5c194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this film seems thirsty for reflection , itsel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the movie's thesis -- elegant technology for t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tries too hard to be funny in a way that's too...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disturbingly superficial in its approach to th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an ugly , pointless , stupid movie .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  this film seems thirsty for reflection , itsel...       1\n",
       "1  the movie's thesis -- elegant technology for t...       1\n",
       "2  tries too hard to be funny in a way that's too...       0\n",
       "3  disturbingly superficial in its approach to th...       0\n",
       "4               an ugly , pointless , stupid movie .       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define file paths\n",
    "neg_file_path = 'rtmr/rt-polarity.neg'\n",
    "pos_file_path = 'rtmr/rt-polarity.pos'\n",
    "\n",
    "# Read files\n",
    "with open(neg_file_path, 'r', encoding='latin-1') as f:\n",
    "    neg_lines = f.readlines()\n",
    "\n",
    "with open(pos_file_path, 'r', encoding='latin-1') as f:\n",
    "    pos_lines = f.readlines()\n",
    "\n",
    "# Create DataFrames\n",
    "df_neg = pd.DataFrame({'text': [line.strip() for line in neg_lines], 'target': 0})\n",
    "df_pos = pd.DataFrame({'text': [line.strip() for line in pos_lines], 'target': 1})\n",
    "\n",
    "# Combine them\n",
    "df = pd.concat([df_neg, df_pos], ignore_index=True)\n",
    "\n",
    "# Optional: shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e523140a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(df[\"text\"], df[\"target\"], test_size=0.2, random_state=RANDOM_STATE, stratify=df[\"target\"])\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ecaa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_csv(\"rtmr/X_train.csv\", index=False)\n",
    "# y_train.to_csv(\"rtmr/y_train.csv\", index=False)\n",
    "\n",
    "# X_val.to_csv(\"rtmr/X_val.csv\", index=False)\n",
    "# y_val.to_csv(\"rtmr/y_val.csv\", index=False)\n",
    "\n",
    "# X_test.to_csv(\"rtmr/X_test.csv\", index=False)\n",
    "# y_test.to_csv(\"rtmr/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ca1a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train samples: 8529\n",
      "Num val samples  : 1066\n",
      "Num test samples : 1067\n"
     ]
    }
   ],
   "source": [
    "X_train_df = pd.read_csv(\"rtmr/X_train.csv\")\n",
    "X_val_df   = pd.read_csv(\"rtmr/X_val.csv\")\n",
    "X_test_df  = pd.read_csv(\"rtmr/X_test.csv\")\n",
    "\n",
    "# Flatten to list of strings\n",
    "X_train = X_train_df.iloc[:, 0].astype(str)\n",
    "X_val   = X_val_df.iloc[:, 0].astype(str)\n",
    "X_test  = X_test_df.iloc[:, 0].astype(str)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Num train samples:\", len(X_train))\n",
    "print(\"Num val samples  :\", len(X_val))\n",
    "print(\"Num test samples :\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3982d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(glove_path=\"glove.840B.300d.txt\"):\n",
    "    embeddings = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove_embeddings = load_glove_embeddings()\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca9e3407",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "with open(\"rtmr/tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "# Convert to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq   = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq  = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_len = 100  # or compute dynamically via np.percentile or np.max\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_val_pad   = pad_sequences(X_val_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_pad  = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38ff05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"rtmr/X_train_pad.npy\", X_train_pad)\n",
    "np.save(\"rtmr/X_val_pad.npy\", X_val_pad)\n",
    "np.save(\"rtmr/X_test_pad.npy\", X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2785c9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1  # +1 for padding token\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    vector = glove_embeddings.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8243f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"rtmr/embedding_matrix.npy\", embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c4d66",
   "metadata": {},
   "source": [
    "# Hate Speech and Offensive Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6344f27",
   "metadata": {},
   "source": [
    "Dispon√≠vel em https://github.com/t-davidson/hate-speech-and-offensive-language/blob/master/data/labeled_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5efb6c1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"hate/labeled_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aadaf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['class'] = data['class'].map(lambda x: 1 if x in [0, 1] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb049c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "1    4163\n",
      "0    4163\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate the two classes\n",
    "class_1 = data[data['class'] == 1]\n",
    "class_0 = data[data['class'] == 0]\n",
    "\n",
    "# Downsample class 1\n",
    "class_1_downsampled = resample(class_1,\n",
    "                               replace=False,\n",
    "                               n_samples=len(class_0),\n",
    "                               random_state=RANDOM_STATE)\n",
    "\n",
    "# Concatenate back\n",
    "balanced_data = pd.concat([class_0, class_1_downsampled])\n",
    "\n",
    "# Optional: Shuffle the dataset\n",
    "balanced_data = balanced_data.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# Final class distribution\n",
    "print(balanced_data['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00d25ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = balanced_data[[\"tweet\", \"class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9b809f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(balanced_data[\"tweet\"], balanced_data[\"class\"], test_size=0.2, random_state=RANDOM_STATE, stratify=balanced_data[\"class\"])\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.to_csv(\"hate/X_train.csv\", index=False)\n",
    "# y_train.to_csv(\"hate/y_train.csv\", index=False)\n",
    "\n",
    "# X_val.to_csv(\"hate/X_val.csv\", index=False)\n",
    "# y_val.to_csv(\"hate/y_val.csv\", index=False)\n",
    "\n",
    "# X_test.to_csv(\"hate/X_test.csv\", index=False)\n",
    "# y_test.to_csv(\"hate/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a47d0888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train samples: 6660\n",
      "Num val samples  : 833\n",
      "Num test samples : 833\n"
     ]
    }
   ],
   "source": [
    "X_train_df = pd.read_csv(\"hate/X_train.csv\")\n",
    "X_val_df   = pd.read_csv(\"hate/X_val.csv\")\n",
    "X_test_df  = pd.read_csv(\"hate/X_test.csv\")\n",
    "\n",
    "# Flatten to list of strings\n",
    "X_train = X_train_df.iloc[:, 0].astype(str)\n",
    "X_val   = X_val_df.iloc[:, 0].astype(str)\n",
    "X_test  = X_test_df.iloc[:, 0].astype(str)\n",
    "\n",
    "# Check shapes\n",
    "print(\"Num train samples:\", len(X_train))\n",
    "print(\"Num val samples  :\", len(X_val))\n",
    "print(\"Num test samples :\", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c899b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dataset(\"hate\", X_train, X_val, X_test, glove_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afdd569",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd80f25b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91b8b654728e456bae0e07e6863717b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebe6ffaef434717b648995d259d8bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7e28e194054a5eab5b226735e71956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd99a8de16247d987e35e166c027dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b640dfb677742bf9bfcc0035d25cad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e910794f43745b29f5e640a6344b4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbade5954314e5e84246d4f8193bab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d7c94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV files saved to folder IMDB\n"
     ]
    }
   ],
   "source": [
    "full_train = dataset[\"train\"]\n",
    "test = dataset[\"test\"]\n",
    "\n",
    "texts = list(full_train[\"text\"])\n",
    "labels = list(map(int, full_train[\"label\"]))\n",
    "\n",
    "# Split training set into train and validation (80/20)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=RANDOM_STATE, stratify=labels\n",
    ")\n",
    "\n",
    "# Extract test set\n",
    "X_test = test[\"text\"]\n",
    "y_test = list(map(int, test[\"label\"]))\n",
    "\n",
    "# Save each split as CSV\n",
    "pd.DataFrame({\"text\": X_train, \"label\": y_train}).to_csv(\"IMDB/train.csv\", index=False)\n",
    "pd.DataFrame({\"text\": X_val,   \"label\": y_val}).to_csv(\"IMDB/val.csv\", index=False)\n",
    "pd.DataFrame({\"text\": X_test,  \"label\": y_test}).to_csv(\"IMDB/test.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ CSV files saved to folder IMDB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b836b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"IMDB/train.csv\")\n",
    "val = pd.read_csv(\"IMDB/val.csv\")\n",
    "test = pd.read_csv(\"IMDB/test.csv\")\n",
    "\n",
    "embed_dataset(\"IMDB\", train[\"text\"], val[\"text\"], test[\"text\"], GLOVE_EMBEDDINGS, max_len=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b46d64",
   "metadata": {},
   "source": [
    "# Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd2151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"kaggle/train_og.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ef720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_columns = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "df[\"label\"] = df[toxic_columns].any(axis=1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61fde6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"comment_text\": \"text\"})\n",
    "df = df[df[\"text\"].apply(lambda x: len(str(x).split()) <= 200)]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "beedad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    15559\n",
      "0    15559\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_0 = df[df[\"label\"] == 0]\n",
    "df_1 = df[df[\"label\"] == 1]\n",
    "\n",
    "# Undersample class 0 to match class 1\n",
    "df_0_downsampled = df_0.sample(n=len(df_1), random_state=42)\n",
    "\n",
    "# Combine balanced data\n",
    "df_balanced = pd.concat([df_0_downsampled, df_1], ignore_index=True)\n",
    "\n",
    "# Shuffle the result\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df_balanced[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d7190eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = df_balanced[[\"text\", \"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "043dbe75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    133925\n",
       "1     15559\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28b90b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 24894\n",
      "Validation: 3112\n",
      "Test: 3112\n",
      "‚úÖ Saved train.csv, val.csv, and test.csv\n"
     ]
    }
   ],
   "source": [
    "train_df, temp_df = train_test_split(\n",
    "    df_balanced, test_size=0.2, stratify=df_balanced[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42\n",
    ")\n",
    "\n",
    "# Check sizes\n",
    "print(\"Train:\", len(train_df))\n",
    "print(\"Validation:\", len(val_df))\n",
    "print(\"Test:\", len(test_df))\n",
    "\n",
    "# Save to CSV\n",
    "train_df.to_csv(\"kaggle/train.csv\", index=False)\n",
    "val_df.to_csv(\"kaggle/val.csv\", index=False)\n",
    "test_df.to_csv(\"kaggle/test.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Saved train.csv, val.csv, and test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbfd0cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"kaggle/train.csv\")\n",
    "val = pd.read_csv(\"kaggle/val.csv\")\n",
    "test = pd.read_csv(\"kaggle/test.csv\")\n",
    "\n",
    "embed_dataset(\"kaggle\", train[\"text\"], val[\"text\"], test[\"text\"], GLOVE_EMBEDDINGS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textbugger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
